{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c409aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:08:28.411298Z",
     "start_time": "2024-04-11T07:07:37.937405Z"
    }
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "from nltk import tokenize\n",
    "from torch.cuda import empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fadaa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import process_iter\n",
    "from signal import SIGTERM # or SIGKILL\n",
    "\n",
    "for proc in process_iter():\n",
    "    try:\n",
    "        for conns in proc.connections(kind='inet'):\n",
    "            if conns.laddr.port == 9003:\n",
    "                proc.send_signal(SIGTERM) # or SIGKILL\n",
    "    except PermissionError:\n",
    "        print(\"Permission error on process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7bd1d2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:30:37.887938Z",
     "start_time": "2024-04-11T09:28:37.410051Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 02:39:04 INFO: Writing properties to tmp file: corenlp_server-02734d9b33bc44bb.props\n",
      "2024-04-26 02:39:04 INFO: Starting server with command: java -Xmx4G -cp C:\\Users\\ivano\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9003 -timeout 120000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-02734d9b33bc44bb.props -annotators tokenize,ssplit,pos,lemma,ner,parse,coref -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "text = 'Science may be forced both ways by the tendency towards colossal quantities of research – both accelerating and trampling its progress'\n",
    "\n",
    "with stanza.server.CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'parse', 'coref'],\n",
    "        timeout=120_000,\n",
    "        memory='4G',\n",
    "        be_quiet=False,\n",
    "        endpoint='http://localhost:9003') as client:\n",
    "    \n",
    "    ann = client.annotate(text)\n",
    "    modified_text = tokenize.sent_tokenize(text)\n",
    "\n",
    "    for coref in ann.corefChain:\n",
    "\n",
    "        antecedent = []\n",
    "        for mention in coref.mention:\n",
    "            phrase = []\n",
    "            for i in range(mention.beginIndex, mention.endIndex):\n",
    "                phrase.append(ann.sentence[mention.sentenceIndex].token[i].word)\n",
    "            if antecedent == []:\n",
    "                antecedent = ' '.join(word for word in phrase)\n",
    "            else:\n",
    "                anaphor = ' '.join(word for word in phrase)\n",
    "                modified_text[mention.sentenceIndex] = modified_text[mention.sentenceIndex].replace(anaphor, antecedent)\n",
    "\n",
    "    modified_text = ' '.join(modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "325a083a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science may be forced both ways by the tendency towards colossal quantities of research – both accelerating and trampling Science progress'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ace158",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 03:54:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a4a13c954f4189830dda46dd01b115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 03:54:47 INFO: Downloaded file to C:\\Users\\ivano\\stanza_resources\\resources.json\n",
      "2024-04-26 03:54:47 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | combined                |\n",
      "| mwt       | combined                |\n",
      "| pos       | combined_charlm         |\n",
      "| coref     | ontonotes_electra-large |\n",
      "=======================================\n",
      "\n",
      "2024-04-26 03:54:47 INFO: Using device: cuda\n",
      "2024-04-26 03:54:47 INFO: Loading: tokenize\n",
      "2024-04-26 03:54:48 INFO: Loading: mwt\n",
      "2024-04-26 03:54:48 INFO: Loading: pos\n",
      "2024-04-26 03:54:49 INFO: Loading: coref\n",
      "C:\\Users\\ivano\\anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2024-04-26 03:54:57 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "pipe = stanza.Pipeline(\"en\", processors=\"tokenize,coref,mwt,pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895e938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The tendency towards colossal quantities of research may force science both ways – both accelerating and trampling its progress. In the context of this paper a concept is defined as an entity and its description(s). '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ff2a632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'In mathematics, a limit is the value that a function (or sequence) approaches as the input (or index) approaches some value.[1] Limits are essential to calculus and mathematical analysis, and are used to define continuity, derivatives, and integrals.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2910de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 875 ms\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a748a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = result.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be95fd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': 1,\n",
       "   'text': 'The',\n",
       "   'upos': 'DET',\n",
       "   'xpos': 'DT',\n",
       "   'feats': 'Definite=Def|PronType=Art',\n",
       "   'start_char': 0,\n",
       "   'end_char': 3,\n",
       "   'coref_chains': []},\n",
       "  {'id': 2,\n",
       "   'text': 'tendency',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 4,\n",
       "   'end_char': 12,\n",
       "   'coref_chains': []},\n",
       "  {'id': 3,\n",
       "   'text': 'towards',\n",
       "   'upos': 'ADP',\n",
       "   'xpos': 'IN',\n",
       "   'start_char': 13,\n",
       "   'end_char': 20,\n",
       "   'coref_chains': []},\n",
       "  {'id': 4,\n",
       "   'text': 'colossal',\n",
       "   'upos': 'ADJ',\n",
       "   'xpos': 'JJ',\n",
       "   'feats': 'Degree=Pos',\n",
       "   'start_char': 21,\n",
       "   'end_char': 29,\n",
       "   'coref_chains': []},\n",
       "  {'id': 5,\n",
       "   'text': 'quantities',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NNS',\n",
       "   'feats': 'Number=Plur',\n",
       "   'start_char': 30,\n",
       "   'end_char': 40,\n",
       "   'coref_chains': []},\n",
       "  {'id': 6,\n",
       "   'text': 'of',\n",
       "   'upos': 'ADP',\n",
       "   'xpos': 'IN',\n",
       "   'start_char': 41,\n",
       "   'end_char': 43,\n",
       "   'coref_chains': []},\n",
       "  {'id': 7,\n",
       "   'text': 'research',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 44,\n",
       "   'end_char': 52,\n",
       "   'coref_chains': []},\n",
       "  {'id': 8,\n",
       "   'text': 'may',\n",
       "   'upos': 'AUX',\n",
       "   'xpos': 'MD',\n",
       "   'feats': 'VerbForm=Fin',\n",
       "   'start_char': 53,\n",
       "   'end_char': 56,\n",
       "   'coref_chains': []},\n",
       "  {'id': 9,\n",
       "   'text': 'force',\n",
       "   'upos': 'VERB',\n",
       "   'xpos': 'VB',\n",
       "   'feats': 'VerbForm=Inf',\n",
       "   'start_char': 57,\n",
       "   'end_char': 62,\n",
       "   'coref_chains': []},\n",
       "  {'id': 10,\n",
       "   'text': 'science',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 63,\n",
       "   'end_char': 70,\n",
       "   'coref_chains': [<stanza.models.coref.coref_chain.CorefAttachment at 0x1ff3e825280>]},\n",
       "  {'id': 11,\n",
       "   'text': 'both',\n",
       "   'upos': 'DET',\n",
       "   'xpos': 'DT',\n",
       "   'start_char': 71,\n",
       "   'end_char': 75,\n",
       "   'coref_chains': []},\n",
       "  {'id': 12,\n",
       "   'text': 'ways',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NNS',\n",
       "   'feats': 'Number=Plur',\n",
       "   'start_char': 76,\n",
       "   'end_char': 80,\n",
       "   'coref_chains': []},\n",
       "  {'id': 13,\n",
       "   'text': '–',\n",
       "   'upos': 'PUNCT',\n",
       "   'xpos': ':',\n",
       "   'start_char': 81,\n",
       "   'end_char': 82,\n",
       "   'coref_chains': []},\n",
       "  {'id': 14,\n",
       "   'text': 'both',\n",
       "   'upos': 'CCONJ',\n",
       "   'xpos': 'CC',\n",
       "   'start_char': 83,\n",
       "   'end_char': 87,\n",
       "   'coref_chains': []},\n",
       "  {'id': 15,\n",
       "   'text': 'accelerating',\n",
       "   'upos': 'VERB',\n",
       "   'xpos': 'VBG',\n",
       "   'feats': 'VerbForm=Ger',\n",
       "   'start_char': 88,\n",
       "   'end_char': 100,\n",
       "   'coref_chains': []},\n",
       "  {'id': 16,\n",
       "   'text': 'and',\n",
       "   'upos': 'CCONJ',\n",
       "   'xpos': 'CC',\n",
       "   'start_char': 101,\n",
       "   'end_char': 104,\n",
       "   'coref_chains': []},\n",
       "  {'id': 17,\n",
       "   'text': 'trampling',\n",
       "   'upos': 'VERB',\n",
       "   'xpos': 'VBG',\n",
       "   'feats': 'VerbForm=Ger',\n",
       "   'start_char': 105,\n",
       "   'end_char': 114,\n",
       "   'coref_chains': []},\n",
       "  {'id': 18,\n",
       "   'text': 'its',\n",
       "   'upos': 'PRON',\n",
       "   'xpos': 'PRP$',\n",
       "   'feats': 'Case=Gen|Gender=Neut|Number=Sing|Person=3|Poss=Yes|PronType=Prs',\n",
       "   'start_char': 115,\n",
       "   'end_char': 118,\n",
       "   'coref_chains': [<stanza.models.coref.coref_chain.CorefAttachment at 0x1ff3e825400>]},\n",
       "  {'id': 19,\n",
       "   'text': 'progress',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 119,\n",
       "   'end_char': 127,\n",
       "   'coref_chains': [],\n",
       "   'misc': 'SpaceAfter=No'},\n",
       "  {'id': 20,\n",
       "   'text': '.',\n",
       "   'upos': 'PUNCT',\n",
       "   'xpos': '.',\n",
       "   'start_char': 127,\n",
       "   'end_char': 128,\n",
       "   'coref_chains': []}],\n",
       " [{'id': 1,\n",
       "   'text': 'In',\n",
       "   'upos': 'ADP',\n",
       "   'xpos': 'IN',\n",
       "   'start_char': 129,\n",
       "   'end_char': 131,\n",
       "   'coref_chains': []},\n",
       "  {'id': 2,\n",
       "   'text': 'the',\n",
       "   'upos': 'DET',\n",
       "   'xpos': 'DT',\n",
       "   'feats': 'Definite=Def|PronType=Art',\n",
       "   'start_char': 132,\n",
       "   'end_char': 135,\n",
       "   'coref_chains': []},\n",
       "  {'id': 3,\n",
       "   'text': 'context',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 136,\n",
       "   'end_char': 143,\n",
       "   'coref_chains': []},\n",
       "  {'id': 4,\n",
       "   'text': 'of',\n",
       "   'upos': 'ADP',\n",
       "   'xpos': 'IN',\n",
       "   'start_char': 144,\n",
       "   'end_char': 146,\n",
       "   'coref_chains': []},\n",
       "  {'id': 5,\n",
       "   'text': 'this',\n",
       "   'upos': 'DET',\n",
       "   'xpos': 'DT',\n",
       "   'feats': 'Number=Sing|PronType=Dem',\n",
       "   'start_char': 147,\n",
       "   'end_char': 151,\n",
       "   'coref_chains': []},\n",
       "  {'id': 6,\n",
       "   'text': 'paper',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 152,\n",
       "   'end_char': 157,\n",
       "   'coref_chains': []},\n",
       "  {'id': 7,\n",
       "   'text': 'a',\n",
       "   'upos': 'DET',\n",
       "   'xpos': 'DT',\n",
       "   'feats': 'Definite=Ind|PronType=Art',\n",
       "   'start_char': 158,\n",
       "   'end_char': 159,\n",
       "   'coref_chains': []},\n",
       "  {'id': 8,\n",
       "   'text': 'concept',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 160,\n",
       "   'end_char': 167,\n",
       "   'coref_chains': []},\n",
       "  {'id': 9,\n",
       "   'text': 'is',\n",
       "   'upos': 'AUX',\n",
       "   'xpos': 'VBZ',\n",
       "   'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "   'start_char': 168,\n",
       "   'end_char': 170,\n",
       "   'coref_chains': []},\n",
       "  {'id': 10,\n",
       "   'text': 'defined',\n",
       "   'upos': 'VERB',\n",
       "   'xpos': 'VBN',\n",
       "   'feats': 'Tense=Past|VerbForm=Part|Voice=Pass',\n",
       "   'start_char': 171,\n",
       "   'end_char': 178,\n",
       "   'coref_chains': []},\n",
       "  {'id': 11,\n",
       "   'text': 'as',\n",
       "   'upos': 'ADP',\n",
       "   'xpos': 'IN',\n",
       "   'start_char': 179,\n",
       "   'end_char': 181,\n",
       "   'coref_chains': []},\n",
       "  {'id': 12,\n",
       "   'text': 'an',\n",
       "   'upos': 'DET',\n",
       "   'xpos': 'DT',\n",
       "   'feats': 'Definite=Ind|PronType=Art',\n",
       "   'start_char': 182,\n",
       "   'end_char': 184,\n",
       "   'coref_chains': [<stanza.models.coref.coref_chain.CorefAttachment at 0x1ff3e8255e0>]},\n",
       "  {'id': 13,\n",
       "   'text': 'entity',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 185,\n",
       "   'end_char': 191,\n",
       "   'coref_chains': [<stanza.models.coref.coref_chain.CorefAttachment at 0x1ff3e825760>]},\n",
       "  {'id': 14,\n",
       "   'text': 'and',\n",
       "   'upos': 'CCONJ',\n",
       "   'xpos': 'CC',\n",
       "   'start_char': 192,\n",
       "   'end_char': 195,\n",
       "   'coref_chains': []},\n",
       "  {'id': 15,\n",
       "   'text': 'its',\n",
       "   'upos': 'PRON',\n",
       "   'xpos': 'PRP$',\n",
       "   'feats': 'Case=Gen|Gender=Neut|Number=Sing|Person=3|Poss=Yes|PronType=Prs',\n",
       "   'start_char': 196,\n",
       "   'end_char': 199,\n",
       "   'coref_chains': [<stanza.models.coref.coref_chain.CorefAttachment at 0x1ff3e8258e0>]},\n",
       "  {'id': 16,\n",
       "   'text': 'description',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': 'Number=Sing',\n",
       "   'start_char': 200,\n",
       "   'end_char': 211,\n",
       "   'coref_chains': [],\n",
       "   'misc': 'SpaceAfter=No'},\n",
       "  {'id': 17,\n",
       "   'text': '(',\n",
       "   'upos': 'PUNCT',\n",
       "   'xpos': '-LRB-',\n",
       "   'start_char': 211,\n",
       "   'end_char': 212,\n",
       "   'coref_chains': [],\n",
       "   'misc': 'SpaceAfter=No'},\n",
       "  {'id': 18,\n",
       "   'text': 's',\n",
       "   'upos': 'ADJ',\n",
       "   'xpos': 'AFX',\n",
       "   'start_char': 212,\n",
       "   'end_char': 213,\n",
       "   'coref_chains': [],\n",
       "   'misc': 'SpaceAfter=No'},\n",
       "  {'id': 19,\n",
       "   'text': ')',\n",
       "   'upos': 'PUNCT',\n",
       "   'xpos': '-RRB-',\n",
       "   'start_char': 213,\n",
       "   'end_char': 214,\n",
       "   'coref_chains': [],\n",
       "   'misc': 'SpaceAfter=No'},\n",
       "  {'id': 20,\n",
       "   'text': '.',\n",
       "   'upos': 'PUNCT',\n",
       "   'xpos': '.',\n",
       "   'start_char': 214,\n",
       "   'end_char': 215,\n",
       "   'coref_chains': []}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "05004c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "resd = {}\n",
    "for a in dicts:\n",
    "    for d in a:\n",
    "        try:\n",
    "            if d['coref_chains']:\n",
    "                if d['coref_chains'][0].to_json()['representative_text'] in resd:\n",
    "                    resd[d['coref_chains'][0].to_json()['representative_text']].append((d['text'], d['start_char'], d['end_char']))\n",
    "                else:\n",
    "                    resd[d['coref_chains'][0].to_json()['representative_text']] = [(d['text'], d['start_char'], d['end_char'])]\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "68393817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "23db3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_text = text\n",
    "list_init = list(range(len(modified_text)))\n",
    "\n",
    "for antecedent in resd:\n",
    "    for mention in resd[antecedent]:\n",
    "        if mention[0] not in antecedent:\n",
    "            extp = f'{mention[0]} (->{antecedent})'\n",
    "            modified_text = modified_text[:list_init.index(mention[1])] + extp + modified_text[list_init.index(mention[2]):]\n",
    "            list_init = list_init[:list_init.index(mention[1])] + [-1]*len(extp) + list_init[list_init.index(mention[2]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7b3fccda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In mathematics, a limit is the value that a function (or sequence) approaches as the input (or index) approaches some value.[1] Limits are essential to calculus and mathematical analysis, and are used to define continuity, derivatives, and integrals.'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a0d45c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tendency towards colossal quantities of research may force science both ways – both accelerating and trampling its (->science) progress. In the context of this paper a concept is defined as an entity and its (->an entity) description(s). '"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "860f52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Article.txt', 'r', encoding=\"utf8\") as f:\n",
    "    article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8335d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "Prediction After a Horizon of Predictability:\n",
      "Non-Predictable Points and Partial Multi-Step Prediction for Chaotic Time Series\n",
      "\n",
      "Vasilii A. Gromov, Philip S. Baranov, and Alexandr Yu. Tsybakin\n",
      "HSE University\t\n",
      "Pokrovskii Boulevard, 11, Moscow, Russian Federation\n",
      "Vasilii A. Gromov, stroller@rambler.ru (corresponding author:)\n",
      "Philip S. Baranov, pbaranov1306@gmail.com\n",
      "Alexandr Yu. Tsybakin, a.tsby@yandex.ru\n",
      "Abstract. The paper introduces several novel strategies for multi-step prediction of chaotic time series. Generalized z-vectors (irregular embeddings), comprising non-successive observations, make it possible to obtain a fairly large set of possible predicted values for each point to be predicted. Upon examining such a set, it may possible either to calculate a unified (‘final’) predicted value for a particular point (thus making it a ‘predictable’ point) or not (in which case the point becomes ‘unpredictable’). With non-predictable points, the multi-step prediction process is, in fact, a two-objective problem: on one hand, we aim to minimise the total number of unpredictable points; on the other – we try to minimise the average error among the predictable ones. The main difference between the strategies presented in this paper and their classical counterparts is the concept of ‘unpredictable’ points – i.e., skipping points where the predicted value will clearly be wrong and calculating predictions only for the ‘good’ points. The results of the present research indicate that, for such algorithms, the number of non-predictable points grows exponentially with a prediction horizon; however, an average error for predictable points tends to remain constant and relatively small up to the horizon of predictability and sometimes even farther (for benchmark and real-world time series).\n",
      "\n",
      "Keywords. Chaotic time series, multi-step prediction, predictive clustering, predictable and non-predictable points, a horizon of predictability.\n",
      "\n",
      "I. INTRODUCTION\n",
      "Chaotic systems, both social and natural, have a widespread currency. This may explain a constantly increasing number of prediction algorithms for chaotic time series. It is worth stressing however, that while the algorithms for a single-step-ahead prediction (Aghabozorgi et al., 2015) appear to be amazingly efficient; their counterparts for a multi-step ahead (MSA) prediction are still in their infancy. One may attribute this fact to an exponential growth of an average prediction error with a prediction horizon, that is the number of steps ahead to be predicted. \n",
      "This exponential growth reflects Lyapunov instability, immanent to any chaotic system. Namely, according to Lyapunov instability definition, however small is any initial difference between two neighbouring trajectories, the difference grows exponentially with time, and its exponent equals to the highest Lyapunov exponent (Kantz & Schreiber, 2003; Malinetskii & Potapov, 2000). Laypunov instability also leads to another concept, that of a horizon of predictability (sometimes also referred to as Lyapunov time (Bezruchko & Smirnov, 2010)). For a given observational error  and the maximum prediction error , the exponential growth mentioned above satisfies, thereby giving a horizon of predictability  (Kantz & Schreiber, 2003; Malinetskii & Potapov, 2000). In the context of a multi-step ahead prediction, this implies that the smallest difference between true and predicted values in an intermediate point (that is the point between the last observable point and the point to be predicted) – inevitable for any prediction method – triggers exponential error growth in all subsequent intermediate points and in the point to be predicted itself. Therefore, a horizon of predictability is an upper theoretical boundary for the number of steps ahead to be predicted for any prediction algorithm (for given  and ). A horizon of predictability should not be confused with a prediction horizon, which is a mere number of steps ahead the algorithm makes its prediction. Usually, a prediction horizon  is essentially lesser than a horizon of predictability : .\n",
      "Nonetheless, this apparently inevitable exponential growth only seems to be so. Predictive clustering (Blockeel & De Raedt, 1998; Gromov & Borisenko, 2015) furnishes several tools that, applied together, make it possible to avoid it.\n",
      "First, predictive clustering uses motifs – repeated sequences of data in a series (with some small variations). If a section of a time series resembles first part of a motif ‘closely enough’, it is likely that the subsequent points of a series will be close to the subsequent points of the motif, allowing one to use motifs to predict next points of the time series. Motifs can be obtained by clustering vectors of non-successive observations of a series into z-vectors (irregular embeddings (Small, 2005)) and calculating their respective centers. It is worth stressing that most available prediction methods attempt to construct a unified, global prediction model, whereas predictive clustering methods, vice versa, build up a set of separate, local prediction models, motifs (cf. global vs. local learning (Ben Taieb et al., 2010; Gromov, 2019)).\n",
      "Second, predictive clustering allows one to develop the concept of non-predictable points in a natural way (Gromov & Borisenko, 2015). Initially, an unpredictable point was the fone for which the algorithm failed to find a corresponding motif that would be close enough and based on which the possible value of the point could be estimated (a set of possible predicted values is empty for it. It is worthy to emphasize that if one introduces the concept of non-predictable points (non-predictables), one thereby renders the problem two-objective: one must minimise simultaneously the number of non-predictable points and the average error over predictable ones. In the conventional statement, the first objective is discarded – predictions are made for every single point. Introduction of unpredictable points proved to have a positive effect on the results – it is better for algorithm to skip some points along the way for which it is unable to find a proper predicted value rather than forcing it regardless of whether this predicted value will be accurate or not. This approach is actually quite natural in some areas: for example, traders are not compelled to buy and/or sell at every single moment – instead, they may choose only those moments in time that the algorithm has recognized as predictable.\n",
      "In the context of dynamical systems (Kantz & Schreiber, 2003; Malinetskii & Potapov, 2000), each cluster of z-vectors (by definition, it includes similar sections of a trajectory) corresponds to a particular area of a strange attractor. Furthermore, areas with high values of an invariant measure are associated with large clusters (i.e., with frequently observed motifs). Similarly, areas with smaller values of the invariant measure are associated with smaller clusters (i.e., rarely observed motifs). Consequently, as a training set increases – so does the number of clusters, while the number of unpredictable points, as well as the average error of the predictable ones, decrease. A large-scale simulation for the Lorenz series (Gromov, 2019) supports this conclusion.\n",
      "Third, Gromov and Borisenko (2015) use generalized z-vectors – vectors comprised of non-successive observations according to certain predefined patterns (irregular embeddings (Small, 2005)). A pattern is defined as a sequence of distances between positions of observations in a series, such that these (non-successive) observations become successive in a generated vector. This concatenated vector generalizes a conventional z-vector (Kantz & Schreiber, 2003; Malinetskii & Potapov, 2000), which corresponds to the pattern  ( times). Figure 1 presents a sample vector constructed according to the pattern   and shows the way z-vectors are clustered into motifs and how the motifs can be used to obtain predictions for the time series at hand. The process of generating a z-vector can be visualized as placing a ‘comb’ with  teeth where tooth  is ‘broken’ and the distances between the remaining  teeth correspond to some pattern. Moving the comb along the series, we, for each position , obtain a vector of observations, to be under the teeth of the comb, . To put it differently, for a given pattern, a conventional sliding window is replaced by a sliding comb with the majority of its teeth broken off. A set of such vectors comprises a sample corresponding to a pattern of    . Each such sample is clustered separately. \n",
      "Fourth, due to the fact that the number of patterns can be significant, so can the number of possible predicted values at each point; however, they are not always statistically independent. This makes it possible to design a great deal of various algorithms to determine a unified prediction value for a set of possible prediction values (for instance, by averaging them). It also becomes possible to extend the concept of the unpredictable points: apart from the points that do not have a corresponding motif that would be close enough, we can also consider points for which it is impossible to calculate a unified prediction value based on the set of possible predicted values to be unpredictable as well. An example case of such a point would be a point which set of predicted values consists of two equally sized clusters with different centres. It is worth stressing that in our previous paper (Gromov & Borisenko, 2015) we employed non-predictable points of the first kind only (a point is non-predictable if there are no motifs close enough to make predictions for it). In the present paper, we employ both kinds of non-predictable points – and this appears to make it possible for the proposed algorithm to predict for so many steps ahead.   \n",
      "It is worth noting that since patterns with a distance between penultimate and ultimate teeth greater than 1, , exist, the fact that a position  is non-predictable does not imply that positions , , etc. would be non-predictable too. This fact is of fundamental importance for multi-step ahead prediction: If one needs to predict a point at a position , one would use the iterative strategy (or rather its modification based on patterns of non-successive observations). Namely, one predicts values for intermediate positions between  and  step by step, identifying unpredictable points along the way and stepping over them. This results in unpredictable sections between predictable positions. We used to refer to the former as a ‘bog’, and to the latter as ‘hillocks’. The entire prediction process is likened to jumping from a hillock to hillock in order to navigate a swamp (or to put it differently – using stepping stones).\n",
      "While the algorithm cannot produce a prediction at each point (thus the word ‘partial’ in the title), it can predict values up to the horizon of predictability and sometimes even further. While the algorithm cannot produce a prediction at each point (thus the word ‘partial’ in the title), it can predict values up to the horizon of predictability and sometimes even further. This paper presents several methods of identifying unpredictable points and corresponding strategies for the multi-step ahead prediction as well as demonstrates partial predictions beyond the horizon of predictability for the benchmark and real-world time series.\n",
      "The rest of the paper is organized as follows. The next section reviews recent advances in the field; the third section formally states the problems under study; the fourth outlines clustering method, and methods employed to obtain predictions and also identify non-predictable points; the fifth section provides the prediction results for a time series generated by the Lorenz system, and a time series of hourly load values in Germany. Finally, the last section presents conclusions.\n",
      "\n",
      "II. RELATED WORKS\n",
      "Recent papers that concern themselves with chaotic time series prediction are quite numerous (Aghabozorgi et al., 2015; Bao et al., 2014; Ben Taieb et al., 2010; Gromov & Borisenko, 2015; Sangiorgio & Dercole, 2020; Ye & Dai, 2019) but most of them only deal with one-step ahead prediction. At the same time, the number of papers tackling the problem of the multi-step ahead prediction (MSA) is considerably lower. This could be attributed to the exponential growth of the average prediction error with an increasing prediction horizon.\n",
      "An MSA prediction algorithm for chaotic time series usually consists of two parts, responsible collectively for the prediction quality. The first is a technique used for a one- or few-step-ahead prediction. The second is a strategy utilized to ‘assemble’ one- or few-step ahead predictions to a multi-step ahead prediction.\n",
      "Algorithms used for a one-step-ahead prediction employs concepts of nearly all fields of data mining and machine learning: support vector regression and its modifications (Bao et al., 2014), multilayer perceptron (Chandra et al., 2017; Mirzaee, 2009), cluster centres in predictive clustering (Gromov et al., 2017; Gromov & Konev, 2017; Gromov & Shulga, 2012; Martínez-Álvarez et al., 2011), LSTM neural networks (Sangiorgio & Dercole, 2020), Voronoi regions partitioning (Kurogi et al., 2014), ridge polynomial neural networks (Waheeb & Ghazali, 2016), wavelet neural networks (Guntu et al., 2020; Ong & Zainuddin, 2019), dilated convolution networks (R. Wang et al., 2020), to name just a few. Dilated convolution networks could be viewed as counterparts of patterns of non-successive observations (see below).\n",
      "Another factor of fundamental importance is the strategies of MSA prediction. The problem can be solved using two basic strategies – the iterated (recursive) strategy and the direct (Ben Taieb et al., 2012; Cox, 1961) strategy. The iterated one implies that the multi-step ahead prediction process is carried out step-by-step, calculating new predicted values based on the predictions made in the previous steps. The direct strategy aims for getting the results immediately, without calculating predicted values for the intermediate positions. The strategy applies prediction techniques for various prediction horizons, thereby providing multiple predictions for a position to be predicted. Taieb, Sorjamaa, and Bontempi (2010) review different methods based on these two basic strategies. Sangiorgio and Dercole (2020) apply both strategies with multilayer perceptrons and LSTM nets employed as tools to predict for a one step ahead.\n",
      "Unfortunately, MSA methods designed in the framework of the aforementioned strategies are, in one way or another, amenable to ‘a curse of an exponential growth,’ discussed above. This gives rise to a number of hybrid strategies aiming to resolve this problem. Ben Taieb et al. (2012), in their brilliant review, compare two basic and three novel strategies (DirRec, MIMO, and DIRMO). DirRec (Direct + Recursive) combines two basic strategies to the effect that it uses the direct approach to predict values, but the number of inputs is enlarged iteratively to include values for just predicted positions. When it comes to the MIMO (Multiple Input Multiple Output) strategy (Universit & Bontempi, 2014), an array of values is produced for the positions between the observed values and a prediction horizon inclusive instead of only a single value corresponding to the prediction horizon. This allows any correlations that the time series in question might have to be stored in the predicted values. \n",
      "Chaotic time series prediction methods that rely upon reservoir computing would mark a different approach (Jaeger, 2004; Lu et al., 2017). Lu et al. (2017) indicate that the method demonstrates excellent results for small prediction horizons; however, their performance severely worsens when applied to larger horizons. Nevertheless, a predicted sequence ‘behaves like the measurements from a typical trajectory on the attractor’ and this approach converges quickly (Canaday et al., 2018). DIRMO (DirRec + MIMO) divides the series up to a prediction horizon into blocks and applies MIMO strategy separately to each block. Authors test these five strategies on a reasonably large sample of different time series (NN5 competition), which reflects various irregularities inherent to time series.\n",
      "Bao, Xiong, and Hu (2014) compare the efficiency of the iterated, direct, and MIMO strategies by performing a one-step-ahead prediction using a modified support vector regression. According to the authors, the MIMO compares favourably with two other strategies (all other factors being equal).\n",
      " Multiple-task learning can be viewed as a kind of strategy for multi-step ahead prediction. Chandra, Ong, and Goh (2017) propose an algorithm to determine a neural network structure to solve MSA prediction problems; their approach can be considered as a combination of the direct and iterated strategies. Wang et al. (2019) utilize a neural model in order to combine periodic approximations for longer periods and machine learning models for shorter periods. This could be viewed as a separate strategy when dealing with data with a marked periodicity. Ye and Dai (2019) employ multi-task learning for multi-step prediction; it is possible to view multi-task learning as a kind of MSA predicted strategy. Kurogi, Shigematsu, and Ono (2014) make use of an out-of-bag model for selecting models for multi-step prediction. Authors aim at predicting a chaotic series as far as it is possible (the largest possible prediction horizon) while retaining reasonable accuracy. Authors present results for the Lorenz series. The present paper discusses a few novel strategies; the main difference from their classical counterparts is non-predictable points, and consequently, an ability to not account for predictions at intermediate positions that are clearly erroneous (Gromov & Borisenko, 2015).\n",
      "\n",
      "III. PROBLEM STATEMENT\n",
      "The paper considers a  -step-ahead prediction problem for a chaotic time series , . We assume that all transient processes in the system that generate the time series have been completed, and the time series reflects the trajectory movement in the neighbourhood of a strange attractor, however complex it can be. The second assumption is that the series meets Takens’ theorem conditions and, consequently, it is possible analyse the attractor structure using time series observations (Kantz & Schreiber, 2003; Malinetskii & Potapov, 2000).\n",
      "A series is divided into two parts:  and , with  being an observable part, used as a training set a prediction model and   being a test set);  , . When the algorithm predicts a value at the  position of the test set, it possesses information about observations from  up to  inclusively, it does not possess information observations about positions .where  is a parameter of the algorithm.\n",
      "Predictive-clustering algorithms and a large number of used patterns make it possible to build up a set of possible predicted values  for each position to be predicted.  is the number of possible predicted values found by an algorithm, where  is the th predicted value. A set  is defined as  and is comprised of sets of possible predicted values for the position  itself and for  preceding positions, with  being a parameter of the algorithm. It usually takes values  or . For , the set  consists of the set of possible predicted values for the position  only; for , it consists of sets for the position  itself and for all intermediate positions between the last observable point and the position . We denote an algorithm that builds up sets of possible predicted values  as : .\n",
      "The concept of non-predictable points implies that one applies two operators to the set . The first checks whether the position is predictable:\n",
      "\n",
      "\n",
      "(1)\n",
      "\n",
      "where  for any function .\n",
      "The second determines a unified predicted value for a set of possible predicted values, provided the position appears to be predictable:\n",
      "\n",
      "\n",
      "(2)\n",
      "\n",
      "In these terms, a multi-step ahead prediction problem can be stated as a two-objective optimization problem:\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "For both functionals, one sums over all observations of a test set. The first functional minimises the number of non-predictable points, while the second one minimizes an average error for the predictable ones. In this paper, authors attempt to solve the two-objective optimization problem (3), (4). \n",
      "\n",
      "IV. PREDICTION ALGORITHM\n",
      "This section describes the proposed prediction algorithm. The first subsection deals with a way to generate samples from the time series according to predefined patterns; the second one talks about the employed clustering techniques. The third, fourth, and fifth provide ways of generating a set of possible predicted values, quality measures of identification of unpredictable points, and techniques of identification of unpredictable points respectively. Each technique in this section is denoted by a label in square brackets (for example, [p]) for ease of reference in the next section – Numerical Results.\n",
      "2.1. Training set\n",
      "The series is considered to be normalized. The concept of patterns is used to generate samples. A pattern is defined as an array of distances between positions of observations such that these (non-successive) observations become successive in a sample vector. Thus, each pattern is a -dimensional vector of integers , ; the parameter  dictates the maximum distance between positions of observations that become neighbours in the vector to be generated. Thereby, the quantity  symbolizes a kind of memory depth.  denotes a set of all possible patterns of length . The vector thus concatenated generalizes a conventional z-vector (Kantz & Schreiber, 2003; Malinetskii & Potapov, 2000), which corresponds to the pattern (1, 1, …, 1) ( times). \n",
      "For example, let us consider a four-point pattern (2, 3, 4). For this pattern, the two first vectors of a training set would be  and ; the last one vector would then be , where  is the last observable value.\n",
      "Conventionally, one used, for predictive clustering, vectors that concatenated successive observations (z-vectors); they proved less efficient (Gromov & Borisenko, 2015) than those based on the vectors concatenated according to various patterns, generalized z-vectors, discussed above. One may attribute this to the fact that vectors of non-successive observations are able to store information about salient observations (minima, maxima, tipping points, and so forth), as well as correlations between them. Figure 2 above shows a pattern superimposed on a time series in order to generate a sample vector.\n",
      "It is possible to use either the entire set of all combinatorically possible patterns  for generating training sets or its part consisting of randomly chosen patterns. The patterns should be chosen before simulation starts. A set of used patterns is denoted as , where  is a percentage of patterns used. In particular,  corresponds to the case of all possible patterns being used. \n",
      "Training sets are generated independently for each pattern . Motifs may be extracted from each training set by two possible ways. The first one utilizes vectors of a training set as motifs [p] (pointwise); this approach bears a close resemblance to lazy learning (Ben Taieb et al., 2012). The second implies that a sample is clustered, and centres of the clusters are used as the motifs in question [cl] (cluster); except as otherwise noted, the clustering technique is the modified Wishart (please, refer to the next subsection). Despite its drawbacks, the first method compares favourably with the second in terms of required computations. \n",
      "We denote a set of motifs corresponding to a pattern ,  as , . The motifs obtained this way can be used only with the respective pattern , which indicates distances between positions in a series, such that values  may occupy. A set of all used motifs is denoted as , . \n",
      "\n",
      "2.2. Clustering technique\n",
      "As the trajectory of a system moves through the same area of an attractor repeatedly, similar sequences (associated with that area) can be repeatedly encountered in the time series. Revealing and describing such areas, as well as developing simple prediction models for each of them, makes it possible to predict chaotic time series up to a considerable limit (Gromov & Shulga, 2012). The clustering method presented below is employed to collect sequences belonging to the same cluster. \n",
      "We employ the Wishart clustering technique (Bock, 1970; Wishart, 1969) as modified by Lapko and Chentsov (Lapko & Chentsov, 2000) to cluster vectors. The method employs graph theory concepts and a non-parametric probability density function estimator of  -nearest neighbours. Some difficulties associated with the application of the algorithm to forecasting problems are discussed in (Gromov & Borisenko, 2015).\n",
      "An estimated saliency for a point  is defined as , where  and  are the volume and radius of the minimum hypersphere with its centre at point  containing at least  observations. The method relies upon a similarity relation (proximity) graph , vertices of which correspond to samples and edges defined as .  is a generated subgraph for graph , with a vertex set  and an edge set that comprises all edges from , such that their vertices belong to the set .  is a cluster number (label) for . A cluster  () is said to be a height-significant one (with respect to height value ) if  \n",
      "Thus, the algorithm consists of the following steps:\n",
      "    1. Determine distances  for each sample to its -nearest neighbor and sort the samples in ascending order according to their respective ;\n",
      "    2. Set ;\n",
      "    3. For each subgraph  the following alternatives are possible:\n",
      "        3.1.  If a vertex  is an isolated vertex of , then start a new cluster;\n",
      "        3.2.  If a vertex  is connected to vertices of clusters :\n",
      "            3.2.1. If all  clusters are completed, then set .\n",
      "            3.2.2. Otherwise, determine the number of significant clusters :\n",
      "                3.2.2.1. If  or  , then set , label significant clusters as completed and delete insignificant clusters, setting  for all samples that belong to them.\n",
      "                3.2.2.2. Otherwise, merge clusters  into , setting \n",
      " for the sample itself and for all samples belonging to them.\n",
      "    4. Set . If , then go to step 3.\n",
      "\n",
      "We used the following values: , .\n",
      "\n",
      "2.3. Sets of possible predicted values ()\n",
      "For a given pattern , , we consider a set of all corresponding motifs  and, using them, builds up a set of possible predicted values  associated with the pattern . Namely, for a given position to be predicted , we compose a vector from time series observations according to the pattern : . All elements of the vector  are assumed to be known – they may be either observed or already predicted values. Then if the Euclidean distance between a vector  and a truncated motif ,  (a truncated motif comprises all elements but the last one,  for ), then the last element of the motif , , is a reasonable estimate of the value to be predicted.\n",
      "Respectively, the set  is determined as \n",
      ", where  is a small threshold. In turn, a set of possible predicted values for a position  is a union of the sets  over all possible patterns, . The unified predicted value  is calculated as a function of the set of possible predicted values for this position .\n",
      "Previously predicted values at intermediate positions  are used as inputs for new iterations of the algorithm until some prediction horizon  is reached. Let us call each iteration a ‘prediction operation’ – a generalized step in the iterated strategy. Once again, it is not necessary to make predictions for all intermediate positions – some may be skipped.\n",
      "A repeated application of this procedure produces an algorithm that implements an operator : , which yields a set of possible predicted values for a position  to be predicted. Since the number of patterns is rather large, so the size of the set may be. This makes it possible to design various algorithms to determine a unified predicted value and identify non-predictable points. These algorithms are discussed in greater detail later.\n",
      "Apart from sets of possible predicted values, we can also calculate a set of weights , , which characterize comparative significance of possible predicted values . We utilize several alternatives to determine weights, based upon the following ideas:\n",
      "\n",
      "    1. The first technique relies on the notion of ‘prediction length’ required to obtain a predicted value . The algorithm introduces some error with each run of the prediction operation, therefore making the result less reliable the more times the prediction operation has been performed. Since the distances between the first and last positions for various patterns may differ significantly for large , the number of prediction operations carried out to obtain various  values may significantly differ as well. To calculate this quantity, we use the following recurrence relation. We assign unity weights  to observed values; then, if  a possible predicted value  is obtained using a pattern  and the values   (observed or predicted) are used as inputs for this prediction operation  have weights , then one calculates the weight of these value as an average of these weights times a step-down factor :\n",
      "\n",
      "(5)\n",
      "\n",
      "We assign average weight of all possible predicted values that take part in its calculation to the unified predicted value. Thus, the step-down factor  ensures that values calculated with a larger number of prediction operations receive smaller weights. In the simulation,  usually takes up a value of  [wl] (weighted sum, length).\n",
      "    2. The second technique suggests that a weight  is inversely proportional to the distance between observed values and the centre of the cluster chosen to make the prediction:\n",
      "\n",
      "\n",
      "(6)\n",
      "\n",
      ", where  is a small parameter used as a threshold for building up a set of possible predicted values. In the simulation,  usually equals  (for a normalized series) [wd] (weighted sum, distance).\n",
      "    3. The third approach suggests that a weight  is a product of weights of the first and second approaches [w] (weighted sum, combined).       \n",
      " \n",
      "2.4. A unified predicted value ( (1))\n",
      "The basic dichotomy for algorithms when determining a unified predicted value is whether  or . If , the unified predicted value is determined using a set of possible predicted values associated with the current position,  [S] (set). If , it also employs the sets associated with preceding positions [T] (trajectory). \n",
      "For the case of , several techniques to determine the unified predicted value for a set of possible predicted values  are possible:\n",
      "    • Calculating an average value:  [av] (an average)\n",
      "    • Calculating a weighted average value:  [wl, wd, or w] (please, see above).\n",
      "    • Clustering a set  in order to select the largest cluster :\n",
      ", , , . The modified Wishart (Wishart, 1969) and DBSCAN (Maronna, 2016) are employed. The latter gives a good account of itself for one-dimensional data, that is fully applicable to a set . Both algorithms do not require a priori information about the true number of clusters. The unified predicted value is calculated as  [cm] (maximum cluster).\n",
      "    4. This technique is similar to the previous one with one exception: we cluster only those elements of , which weights are greater than some threshold . Alternatively, one may discard  percent of observations with the least weights, regardless of the weights absolute values [cw] (a maximum cluster, weighted). \n",
      "    5. The two last techniques can be readily extended to the fuzzy sets. Normalized weights could be viewed, quite naturally, as values of a membership function to belong to a set of possible predicted values. A clustering algorithm should be replaced by some algorithm that can cluster fuzzy data [fs] (a fuzzy set). \n",
      "    6. Roulette wheel. Similarly to the previous techniques, we cluster a set of possible predicted values, but probabilistically choose the cluster which centre will be used as the unified predicted value. Probability for a cluster to be chosen is directly proportional to the number of elements it contains or to the normalized sum of weights associated with its elements [rw] (a roulette wheel). \n",
      "    7. Most frequent value. We divide a range of values of the series in question, into disjoint intervals of equal lengths : , , . An average value calculated over the elements of the most ‘populated’ interval, , is the unified predicted value [mf] (the most frequent).\n",
      "    8. Randomly perturbed most frequent value. This approach is essentially the same as the previous one, with the exception of adding a normally distributed noise to the average value:  , where  is a normally distributed random variable with zero mean and variance  [mp] (the most frequent, perturbed).    \n",
      "\n",
      "When , we utilize the concept of a ‘possible predicted trajectory.’ A possible predicted trajectory is defined as a sequence of possible predicted values at positions (not necessarily successive) preceding a prediction horizon and at the prediction horizon itself. In the framework of this approach, instead of producing sequence of sets of possible predicted values, the prediction method produces a set of possible trajectories that end in a prediction horizon. For some methods, the trajectory is built by adding a random component at each position, thus making the maximum number of trajectories a parameter of the prediction algorithm. For other methods, several values are chosen at each point instead of a single unified prediction value, thereby making a ‘fan’ of trajectories. Obviously, the number of trajectories grows exponentially, and in order to avoid combinatorial explosion, quantitative constraints should be introduced (this could be visualized as placing the trajectories inside some kind of a ‘pipe’).\n",
      " denotes an th possible predicted trajectory; , its value at an th position; , the maximum number of trajectories.  denotes a set of all trajectories that end at a position ; , a set of their values at an th position. Quite naturally, . It should be noted that that since a trajectory may not contain all successive points, generally a set  does not coincide with a set .\n",
      "In the case of , some techniques to build up sets of possible predicted trajectories and then determine a unified predicted value are possible:\n",
      "\n",
      "    1. Randomly perturbed trajectories. This technique employs a conventional iterative strategy (predicted values are calculated step by step at all intermediate positions). A unified predicted value is calculated for each trajectory independently, with the employment of its own sets of possible predicted values. This value is equal to a perturbed average value over elements of the largest cluster (cf. the technique № 8 of the previous list): \n",
      ", where  is a normally distributed random variable with zero mean and variance . In general, this technique closely resembles the respective technique for , but it requires that the possible predicted trajectory be built times. Figure 3 shows a number of possible predicted trajectories for the Lorenz series [tp] (a trajectory, perturbed). \n",
      "    2. Trajectories that rely on a choice of several clusters (the garden of forking trajectories; a fan of trajectories). We cluster the sets of possible predicted values at each intermediate position and considers the following centres of the clusters similarly to the second technique of the previous list:\n",
      ",  . In this case, however, we choose several centres, thus making trajectories to branch. Usually, the first centre is that of the largest cluster with the rest being centres of the second largest cluster, the third, etc. and/or being centres of randomly chosen clusters. If all clusters are chosen randomly, we choose among clusters larger than a certain predefined threshold. Regardless of the way the centres are chosen, each one becomes the next element of a separate trajectory. In order to cut the number of the trajectories employed, we introduce constraints based on their weights. A weight of a trajectory is defined as a product of weights of its elements, which may be calculated by any of the three ways discussed above. In other words, a ‘fan’ of trajectories is transformed into a ‘pipe.’ For the cutting procedure, the current length (the number of elements) of trajectories should exceed a certain predefined threshold. As above, one may exclude trajectories with weights less than a certain value  or  percent of ‘lightest’ trajectories, regardless of their actual weights [tp] (a trajectory, multiple clustering).\n",
      "    3. Trajectories that rely on reduced initial information. The technique combines, in a sense, iterated and direct strategies (Ben Taieb et al., 2012). It builds principal trajectory, starting at point  as well a series of additional trajectories starting at points , ,…,, where  is a parameter. For an additional trajectory, observed values after its starting point are neglected and replaced by predicted values, calculated by the prediction algorithm in a usual way [tr] (a trajectory, reduced information).            \n",
      "For all techniques, a unified predicted value is calculated as an average over the last values of the above trajectories: . It is worth noting that the second technique appears to be the most efficient, at the sacrifice, quite naturally, of computational efficiency.  \n",
      "Interestingly, for the predictive-clustering algorithms discussed in this study, a set of non-predictable points appears to closely approximate a set of points for which the same algorithm without a procedure that identifies non-predictable points fails to make at least a satisfactory prediction. This observation provides the basis for a strategy to design various non-predictable-points identifying techniques.  \n",
      "\n",
      "2.5. Quality measures of unpredictable points identification algorithms\n",
      "The prediction method under discussion utilizes various techniques for identifying unpredictable points. They can either use sets of possible predicted values for a position to be predicted  () or a set of possible predicted trajectories that end at this point  (). For each prediction horizon  we may consider a set \n",
      "; then its set of non-predictable intermediate position is defined as \n",
      ". A set of all non-predictable points of a test set is defined as\n",
      ". This set consists of all positions of a test set such that the prediction algorithm fails to predict, when it predicts  steps ahead. It is important to note that a point may belong to the set of non-predictable points if: \n",
      "    1. Its set of possible predicted values is empty;\n",
      "    2. This set comprises predicted values such that an adequate prediction value is impossible to obtain based on them.\n",
      "The functional (3) in effect minimizes the size of this set. \n",
      "Sets like this depend heavily on the algorithm used for a one-step-ahead prediction, , a value of , and a technique used to identify non-predictable points . To design baselines for non-predictable-points identifying algorithms – and to compare them (for a given) – we consider two extreme cases. For the first one, we do not employ a non-predictable-points identifying algorithm altogether: ,  by default. It means the prediction algorithm uses the closest motif regardless of how remote it is [fp] (forced prediction). A kind of light version of this baseline does not use motifs that are farther than a certain threshold [ab]. This version (the first extreme case) serves as a baseline for other. \n",
      "For the second extreme case, we assume that the prediction algorithm possesses a priori information about observed values at intermediate positions. More precisely, if the difference between its unified predicted value and the true value at the same position is more than  (a small parameter then the algorithm marks this point as unpredictable. Note that the algorithm does not replace its predictions by the true values – instead, it simply does not take clearly erroneous predictions into account during next prediction operations. Such points comprise, for a given prediction algorithm, its set of the ground-truth non-predictable points. Namely, for each point  that we are trying to predict, we can build up the set\n",
      ", where  is, as usual, a unified predicted value calculated for the set of possible predicted values . Hereinafter, the distance is Euclidean. Then, similarly to the above, its set of the ground-truth non-predictable intermediate points is defined as\n",
      "  \n",
      "and the set of the ground-truth non-predictable points for a test set, as .\n",
      "Attempting to predict at  and excluding all intermediate points that fall into the set , leads to the second extreme case – a prediction method that uses an algorithm identifying unpredictable points using a priori information (in our internal team slang, this algorithm is named ‘the daemon’ – Socrates is known to have had his own daemon, who gave him advice in his most painful situations Wide-raging simulation reveals that the results of such an algorithm are nearly independent of a threshold value . Quite naturally, there is no such things for real-world algorithms: they do not possess information about the true values for intermediate positions , and those pieces of information they have at their disposal are either sets of possible predicted values  or sets of possible predicted trajectories . Nonetheless, this algorithm is extremely useful in determining the lower error boundary reachable by any other prediction algorithm. Algorithms identifying unpredictable points should be developed in a way that would make their results as close as possible to this boundary. Graphs in Fig. 4 exemplify dependences of the number of non-predictable points (Fig. 4 а) and the average error on predictable points (Fig. 4 b) on the prediction horizon  for the two extreme cases. The blue curves correspond to the first extreme case (intermediate non-predictable points are not identified at all), and orange, to the second (intermediate non-predictable points are identified with a priori information). It can be seen that in the first case the number of unpredictable points is equal to zero and the prediction error grows exponentially with . The second extreme algorithm demonstrates the opposite: the number of non-predictable points grows exponentially with , while the prediction error function remains mostly constant, bounded, and rather small. It is obvious that the first algorithm minimizes the functional (3), neglecting the functional (4); the second one minimizes the functional (4), neglecting the functional (3). Figures 5a and 5b display the true values (blue solid) and intermediate predicted values (red dashed) for the Lorenz series for the first (5 a) and second (5 b) extreme cases. From the latter subfigure we notice that the second algorithm does not make a prediction for all points, while making rather accurate predictions where it ‘decides’ to predict. On the other hand, we notice that in the first case the predicted trajectory diverges from the true one just after the point (a green disk in Fig. 5 a) that is identified as non-predictable by the second algorithm – the first algorithm must predict at this point (as with any point), and this immediately ruins the MSA prediction process, causing the predicted trajectory to diverge from the true one.   \n",
      "Any other prediction algorithm is a trade-off between these two extreme algorithms. In the context of multi-step ahead prediction, the second case is essentially more interesting, since it allows one to predict a fairly large number of steps ahead. Consequently, all algorithms discussed below attempt to follow suit with this very algorithm (in the team’s internal slang, we referred to them as ‘approximating the daemon’ :).) \n",
      "Figure 5 c displays the true Lorenz series (blue), predicted values by Prof. Kurogi and colleagues (green), and the values predicted by the algorithm discussed in the present paper (red dashed line with disks). We should stress that at  the trajectory predicted by Prof. Kurogi and colleagues diverges from the true trajectory, while our algorithm identifies several non-predictable points. This makes it possible to predict essentially farther and find ‘hillocks’ of predictable points farther down from the last known point.\n",
      "Common sense (and large-scale simulation) suggests that such an algorithm will work efficiently if and only if a set of identified non-predictable points  is sufficiently close to the set of the ground-truth non-predictable points . Symmetric difference of the two sets constitutes an auxiliary quality measure for a technique identifying non-predictable points:\n",
      "\n",
      "\n",
      "(7)\n",
      "\n",
      "Two opposite situations can be observed: \n",
      "    1. The difference  is large, whereas the difference  is small;\n",
      "    2. The difference  is large, whereas the difference  is small.\n",
      "The first case seems to correlate with an overly optimistic algorithm while the second correlates with an overly conservative one.\n",
      "\n",
      "2.6. Algorithms to identify non-predictable points (function  (1))\n",
      "These algorithms show a basic dichotomy between those that employ a set of possible predicted values  () and those that employ a set of possible predicted trajectories  (). In order to gain a better insight into the identification algorithms, we should consider a ‘paragon’ of a predictable point. We believe that its set of possible predicted values should consist of a single compact cluster, which includes an overwhelming majority of elements, and a number of small clusters and/or individual elements. On the contrary, non-predictable points are associated either with a continuum of possible predictable values not amenable to clustering or, vice versa, with several approximately equinumerous compact clusters. In terms of probability distributions, a predictable point correlates with a unimodal symmetric distribution with relatively small variance, while the unpredictable one correlates either with a nearly uniform distribution or with a distribution with several pronounced modes.\n",
      "In order to characterize unpredictable points, we employ the following quantities:\n",
      "    • Multimodality – a percent of possible predicted values remote from the main mode;\n",
      "    • Difference between  and  percentiles;\n",
      "    • Second, third, and fourth central moments, as well as the kurtosis of the distribution and its entropy.\n",
      " We performed large-scale simulation on the validation set  by calculating sets of unpredictable points  and the ground-truth unpredictable points  for each feature separately. A comparison of these sets makes it possible to estimate the best threshold values for each feature. It also reveals that each feature (except entropy) by itself does not lead to larger values of objective (7) [en] (entropy).\n",
      "In terms of clustering of a set of possible predicted values, we employ the following quantities:\n",
      "    • Number of clusters;\n",
      "    • Percent of possible predicted values that fall into the largest cluster;\n",
      "    • Difference between the percentage of values belonging to the largest and smallest clusters.\n",
      "Similarly to the previous case, it is revealed that these quantities do not ‘work’ separately either. Consequently, the main course would be to use them together with quantities based on the distribution of possible predicted values.\n",
      "We employ the following methods to separate the features mentioned above into regions corresponding to predictable and unpredictable points:\n",
      "    1. Logistic regression [lr];\n",
      "    2. Support vector machine [sv];\n",
      "    3. Decision tree c 4.5 [dt];\n",
      "    4. K-nearest neighbours clustering [kn];\n",
      "    5. Multilayer perceptron with a single layer and 8 neurons in it [mp].\n",
      " Mathematically, in order to apply this approach, a third validation set , independent of the training set  and the testing set  should be introduced:: , . When training these models, we label the validation set using the set of ground-truth unpredictable points. Since the number of unpredictable points is larger than the number of predictable ones for larger prediction horizons, a sample is balanced by SMOTE methods (synthetic minority over-sampling technique) (Chawla et al., 2002). This method generates new elements in the neighbourhood of the smaller cluster.\n",
      "In addition to the above classifying methods, we employ their ensembles:\n",
      "    1. AdaBoost (Adaptive Boosting) – an algorithm implying that each subsequent classifier is trained on the elements incorrectly classified by the current classifier [al or as] (adaptive boosting for logistic regression or support vector machine, respectively);\n",
      "    2. Stacking – an algorithm implying that several classifiers are applied in order to produce a feature space for a combined algorithm (logistic regression) [sl] (stacking, logistic regression);\n",
      "    3. Voting – an algorithm that assigns a label to each element by the classifiers’ votes [vl] (voting, logistic regression).\n",
      " For the second approach, when analysing a set of possible predicted trajectories , trajectories converging at a certain position indicates predictability. Plots in Fig. 5 exemplify the set of possible predicted trajectories (thin green lines); a solid blue line shows the true trajectory for comparison; red disks denote predictions made by the algorithm (all other points are identified as unpredictable). From this figure we notice that predictable points are separated by sections of non-predictable points. On the other hand, predictable points are associated with regions where possible predicted trajectories converge. It is worth noting that predicted values are in fairly good agreement with the true ones at the points where algorithms identified as predictable: the results shown in the figure correspond to a conservative variant of the algorithm; the algorithm seeks to minimize average error over the predictable points (the objective  (4)), and to a lesser degree the number of non-predictable points (objective  (3)). Figure 6 displays a true trajectory, a predicted trajectory, and sets of possible predicted values at intermediate positions. The figure illustrates the fact that a large spread (larger than some average value) of a set of possible predicted points suggests the existence of a divergence between true and predicted trajectories. On the other hand, several spaced out clusters may be observed at points like this instead of a single compact cluster.\n",
      "These observations were utilized to develop several techniques to identify non-predictable points:\n",
      "    1. We apply the prediction algorithm to a validation set  in order to calculate an average spread over sets of possible predicted values\n",
      " . Then the algorithm recognizes a point as non-predictable, if its spread exceeds this average value times some factor  [ls] (large spread).\n",
      "    2. Unlike the previous case, one considers not the spread itself \n",
      ", but its variation over some successive positions. If it increases monotonically, then the first point is classified as non-predictable. Simulation suggests that the best option here is to consider three successive points. It may be noted (refer to Fig. 6) that the divergence of true and predicted trajectories correlates with a monotonic spread growth (indicated by pink dashed lines) [rg] (rapid growth). One may also note growth of the number of possible predicted value clusters obtained with clustering algorithms (DBSCAN [rd] or the Wishart clustering [rw]).\n",
      "    3. For the trajectories relying on reduced initial information, we may compare the last value of a trajectory starting at a position , , with the weighted average of the last values for trajectories starting at positions , ,…,: . If  is greater than small , then the point is recognized as non-predictable. The weights are ; hence trajectories starting from positions closer to  possess larger weights [ca] (compare with average). \n",
      "    4. Another variant, designed for the trajectories that rely on reduced initial information implies comparing an averaged modulus of the difference between the last points of a trajectory starting at a position  and a trajectory starting at other positions  with  [ad] (averaged difference). \n",
      "    5. Similarly to the previous case, we calculate a weighted average of the differences  with weights equal to \n",
      " [wa] (weighted average).\n",
      "    6. It is also possible to apply all techniques from the previous list to a set of final points of the trajectories  [wd] (weighted, difference). \n",
      "For comparison, tables of the next section include results for the extreme cases. Namely, a dash symbolizes that non-predictable points are not identified altogether, predicted values are forcibly calculated at all intermediate points; ab (absent) implies that a set of non-predictable points consists only of the points that the algorithm failed to find motifs close enough (), sets of possible predicted values are not analysed (the first extreme case); id (ideal), sets of possible predicted values are constructed using a priori information (the second extreme case).\n",
      "\n",
      "V. NUMERICAL RESULTS\n",
      "\n",
      "The approach discussed in the previous section is applied to both benchmark (Lorenz) and the real (hourly electricity load) time series. The clustering algorithm is applied to generate samples using all possible patterns of four elements () with the maximum distance between neighbouring positions in the pattern equal to 10. Thus, the total number of patterns amounts to =1000. If the case when algorithm does not use all possible patterns, the percentage of used patterns is equal to 4 %, =40.\n",
      "We utilize two types of plots for each series. The first type (see Appendix A) shows a predicted trajectory up to a prediction horizon (a dashed red line with disks of predicted values); a true trajectory is shown for comparison (a solid blue line). For the predicted trajectory, the first presented value corresponds to a one-step ahead prediction, the second – to a two-step ahead prediction, and so on Plots of the second type display an averaged (over a test set) error measure (the number of non-predictable points, the root-mean-square error [RMSE], the mean absolute percentage error [MAPE]) against a prediction horizon. Each such figure also contains, for comparison, the respective plots for two extreme cases (refer to Section Quality measures for algorithms to identify non-predictable points) for the algorithm that does not identify non-predictable points (blue) and the one that identifies them using a priori information (orange line normally, or red in case of 4% of patterns being used). The two plots make a ‘fork’ that usually bounds from below and above plots of all other discussed algorithms.       \n",
      "These results are also presented in tabular form for several typical prediction horizons . We used two types of tables. The first four columns present characteristics of the method used and are the same for both types; namely, the first column indicates a percentage of employed patterns  and the way the algorithm obtains motifs (refer to the Section A training set). The second column indicates whether the algorithm builds up a set of possible predicted values or predicted trajectories and the way it calculates a unified predicted value (refer to the Sections Sets of possible predicted values and A unified predicted value). The third column provides information on the way the algorithm identifies non-predictable points (refer to the Section Algorithms to identify non-predictable points). For information on symbolic labels used in these columns, please refer to the previous section (mentioned in the square brackets after the corresponding technique) or to tables in Appendix B. The fourth column indicates the figure that displays results obtained using the respective variant of the algorithm and the colour used to plot the respective curves. Finally, all other columns present error measures for prediction horizons , 10, 50 and 100 (the number of non-predictable points, the mean absolute percentage error, and the root-mean-square error). It is worth noting that the first ten lines correspond to the extreme cases (refer to the Section Quality measures for algorithms to identify non-predictable points).\n",
      "Tables of the second type present information on the sets of non-predictable points for various variants of the method considered. The first columns are the same as the respective columns of the tables of the first type. Other columns indicate, (, 10, 50 and 100) , and F-measure\n",
      " for the same prediction horizons.\n",
      "The Lorenz system (Atlee Jackson, 1985; Malinetskii & Potapov, 2000) with standard ‘chaotic’ parameters \n",
      ", integrated using the Runge-Kutta fourth-order method (an integration step is equal to ), yields a time series hereinafter referred to as the Lorenz series. The series in question is a typical chaotic series; it is used as a conventional benchmark to test forecasting procedures for chaotic time series.\n",
      "To distinguish chaotic and noisy time series, Rosso et al. (Rosso et al., 2007) proposed checking the position of a pair of quantities – entropy and complexity – on the appropriate plane. They calculated these quantities for the Lorenz series and got 0.68 and 0.45, thus making it possible to classify this series as chaotic. On the other hand, the highest Lyapunov exponent  for this series, calculated using the Eckman algorithm (Eckmann et al., 1986), is equal to 0.92, which is in a good agreement with results by Malinetskii and Potapov (Malinetskii & Potapov, 2000) (see p. 217). A strictly positive value of this quantity also speaks in favour of the series inherently chaotic nature. \n",
      "Numerical error for the fourth-order Runge-Kutta method amounts to \n",
      ". If the maximum prediction error is chosen as , then estimating a horizon of predictability would yield , or (in integration steps) 75 time series observations. \n",
      "For the Lorenz series, the first 3000 observations are discarded in order to ensure that trajectory moves in the neighbourhood of the respective strange attractor. The test set for the series consists of 1000 observations; the training set consists of 10000. Table I and II are the first and second type tables for the Lorenz series.\n",
      "Large-scale simulation reveals that for the first extreme case (a point is recognized as non-predictable only if there are no motifs close enough to make prediction, ) both the number of non-predictable points and average errors for predictable ones grow exponentially with a prediction horizon. Figure 7 demonstrates the number of non-predictable points (7a), RMSE (7b), and MAPE (7c) as a function of a prediction horizon in order to compare results obtained for the methods utilizing 100% and 4% of patterns. The green and red curves correspond to the method that employ the Wishart clustering (100% and 4%, respectively); orange and magenta correspond to those that do not employ any clustering algorithms (100% and 4%, respectively). Such results allow us to conclude that the prediction quality increases with the number of used patterns: the percent of non-predictable points is nearly zero for a larger prediction horizon, whereas the error measures for predictable points are comparable. \n",
      "Figure 8 presents the results for multi-step ahead prediction (the number of non-predictable points (8a), RMSE (8b), and MAPE (8c)); all methods does not identify non-predictable points altogether (4% of patterns is used). A unified predicted value is determined as an average (blue lines) [av] or a weighted average with weights inversely proportional to a distance to a cluster centre [wd], weights proportional to predictive length [wl], and combined weights [w] (green, orange, and cyan lines, respectively). For comparison, red lines represent the second extreme case (non-predictable points are identified with the employment of a priori information).  It may be concluded that the way the algorithm calculates a unified prediction value has little effect on the rate of this growth. Consequently, of crucial importance is the algorithm used to identify non-predictable points. In what follows, we discuss results for various identification techniques. \n",
      "The first identification technique employs entropy of possible predicted values distribution. Simulation reveals that the optimal threshold between the entropies of predictable and non-predictable points is equal to . Figure А1 (refer to Appendix A) portrays a typical empirical distribution for entropy for the ground-truth non-predictable points  and its complement to a testing set . It is quite obvious that entropy values correlating with the ground-truth non-predictable points and those correlating with predictable ones differ significantly. Unfortunately, it appears that it is impossible to design an efficient test based solely on entropy values. \n",
      "A number of identification techniques apply various machine learning algorithms to feature spaces build for sets of possible predicted values. Figure A9 summarizes the following results:\n",
      "    • Red curves correspond to the unpredictable points identified using a priori information;\n",
      "    • Blue points are not identified at all;\n",
      "    • The green ones are identified using a logistic regression classifier [lr];\n",
      "    • Orange ones – using an SVM classifier [sv];\n",
      "    • Cyan – decision tree [dt];\n",
      "    • Black – k-nearest neighbour [kn];\n",
      "    • Grey – multi-layer perceptron classifier[mp];\n",
      "    • Magenta – Adaboost ensemble of LogReg classifiers [al];\n",
      "    • Yellow – Adaboost ensemble of SVM classifiers [as];\n",
      "    • Pink – voting of LogReg classifiers [vl];\n",
      "    • Orange –stacking of LogReg classifiers [sl].\n",
      "A multi-layer perceptron contains 8 hidden layers and utilizes sigmoidal activation function; support vector machine employs polynomial kernel functions; C4.5 employs entropy criterion without any restriction on tree depth; k-nearest neighbour value is equal to 3.\n",
      " \tAmong all considered machine learning methods, the ones based on logistic regression [lr] (green curves) and Adaboost ensemble of them [al] (magenta curves) stand out. They have a nearly constant dependence of both error measures on non-predictable points on the horizon, and their error measures for predictable points are comparable with those for the algorithm that used a priori information to identify non-predictable points (the second extreme case, red curves). Unfortunately, it also demonstrates an exponential growth of the number of non-predictable points with a rate larger than that of the second extreme case. The method that employs a multi-layer perceptron [mp] (gray curve), vice versa, demonstrates a moderate rate of growth of the number of non-predictable points with rapid error growth. Figures A2-A5 show predicted trajectories compared with a true one for these non-predictable-points identifying techniques. All methods that use an ensemble of classifiers to distinguish predictable and non-predictable points show comparable efficiency. Figure 10 displays results for the algorithms that use growth of the standard deviation value [rg] (green); of the number of DBSCAN clusters [sd] (orange); of the number of Wishart clusters [sw] (cyan) over three consecutive points to identify non-predictable points. Unfortunately, these approaches do not demonstrate desirable behavior. Figure A6 demonstrates true and predicted trajectories for this case.\n",
      "\n",
      "In order to implement an approach using branching trajectories (the ‘pipe’ concept), we cluster points using DBSCAN algorithm and then select those clusters that appear to be larger than = 5% of the total number of possible predicted values. Of these clusters, we chose two, using the following strategies:\n",
      "    1. Choosing the largest and the second largest clusters;\n",
      "    2. Choosing two randomly chosen clusters;\n",
      "    3. Choosing the largest and a number of randomly chosen clusters.\n",
      "The maximum allowed number of trajectories (pipe diameter) is equal to 20. It was ascertained that exponential error growth of the number of predictable points is peculiar to this technique, though it is not as steep as in the previous case. \n",
      "Simulation reveals that the methods based upon sets of predictable trajectories compare favourably to those based upon sets of predictable points. Figure 9 exhibits results for trajectories with random perturbation [tp]. The presented results correspond to a variance of the perturbation equal to 0.05. In order to identify non-predictable points, the algorithm clusters a set of final points of the predicted trajectories. If a size of the largest cluster is less than 4% of the total number of the final points, then the algorithm recognizes this point as non-predictable. To cluster points, we use DBSCAN with the following parameters: eps = 0.01, min_samples = 5. (Orange curve corresponds to the method that does not identify non-predictable points; it differs significantly from the respective curves of the previous plots (the red ones), since the algorithm builds up sets of possible predictable values in different manner). Figure A8 presents typical true and predicted trajectories for this case. \n",
      "An inspection of the figure indicates that the algorithm predicts values at positions up to , that significantly exceeds 75 – an estimated horizon of predictability for the Lorenz series (see above). This justifies the title of the present article. Figure A8 (refer to Appendix A) shows typical curves of error measures against the number of possible trajectories (100 % of patterns are used; no clustering technique is applied). The algorithm identifies non-predictable points when possible predicted trajectories diverge. Orange curves correspond to the algorithm that identifies non-predictable points with a priori information; blue, to the one that does not identify such points (). Results for the methods that use reduced initial information are presented in Fig. 10.\n",
      "\n",
      "The first column indicates a percentage of used patterns  and the way the algorithm obtains motifs (refer to the Section A training set). The second column indicates whether the algorithm builds up a set of possible predicted values or predicted trajectories and how it calculates a unified predicted value (refer to the Sections Sets of possible predicted values and A unified predicted value). The third column provides information on the way the algorithm identifies non-predictable points (refer to the Section Algorithms to identify non-predictable points. For information on symbolic labels used in these columns, please, refer to the previous section (in square brackets after the corresponding technique) or to the tables in Appendix B. The fourth column indicates the figure that displays results obtained using the respective variant of the algorithm and the colour used to plot the respective curves. Finally, all other columns present error measures for prediction horizons , 10, 50 and 100 (the number of non-predictable points, the mean absolute percentage error, and the root-mean-square error). Bold green colour indicates the best results after a horizon of predictability obtained with sets and trajectories of possible predicted values.\n",
      "\t\n",
      "\n",
      "\tLegends of the first 4 columns are the same as in Table I. Other columns indicate results for the prediction horizons of , 10, 50, 100, Recall, Precision, and  -measure.\n",
      "\n",
      "The second series considered is a real-world time series (hourly load values in Germany, from 23:00 12/31/2014 to 14:00 20/02/2016 https://www.entsoe.eu/data/power-stats/). A pairing of entropy and complexity for this series amounts to (0.499, 0.372), indicating chaoticity. Its highest Lyapunov exponent amounts to , which supports the hypothesis that the series is chaotic. \n",
      "All of the aforementioned methods were used in the analysis of this time series; results are demonstrated in tables 3 and 4, which are similar to tables 1 and 2 for the Lorenz series. We present essentially lesser number of plots for the sake of conciseness.\n",
      "\n",
      "\n",
      "\n",
      "Figure 11 demonstrates a section of the time series. Figure 12 demonstrates correlation between the number of non-predictable points and errors along with the corresponding prediction horizon for using trajectories of possible forecasted values [rd]. Figure 13 shows the same graphs when using a set of possible predicted values. Both graphs correspond to the best ways of calculating final predicted values and identifying unforecastable points amongst their respective classes. The best method is the pointwise using 100% of templates with perturbed trajectories (100p tp).\n",
      "We have recreated a simulation by Prof. Kurogi and his colleagues (2014) (multi-step ahead prediction of chaotic time series and out-of-bag estimate of the prediction performance for model selection). Figure 5c above displays the true Lorenz series (blue), predicted values by Prof. Kurogi and colleagues (green), and the values predicted by the algorithm discussed in the present paper (red dashed line with disks). The algorithm discussed appears to provide reliable predicted values at positions essentially farther (from the last observed value) than that by Prof. Kurogi and his colleagues, but it does not make predictions for all positions.\n",
      "\n",
      "VI. CONCLUSIONS\n",
      "Several conclusions may be reached:\n",
      "\n",
      "The paper discusses several novel strategies for multi-step prediction of chaotic time series. Generalized z-vectors, comprising non-successive observations, make it possible to obtain a fairly large set of possible predicted values for each point to be predicted. Upon examining such a set, it may be ascertained whether it is possible to produce a unified predicted value for it or not (whether the point is predictable or non-predictable), and determine this unified value if it is predictable. With non-predictable points, one may state the partial multi-step prediction problem as a two-objective problem: the first functional minimizes the number of non-predictable points, while the second minimizes the average error for predictable ones.\n",
      "It appears that for such algorithms the number of non-predictable points grows exponentially with a prediction horizon, but an average error for predictable points remains nearly constant and rather small. The strategies discussed allow a predictive-clustering algorithm to predict positions after a horizon of predictability for a benchmark (the Lorenz) and a real-world time series.\n",
      "It was concluded that the exact procedure of calculating the final forecasted value has little effect on the accuracy of forecasting, while an effective method of identifying non-predictable points dramatically expands the horizon of prediction. It is worth stressing that that this effect is possible only due to the fact that an algorithm examines sets of non-predictable points and determines whether it possible to calculate a unified prediction value for it (the second kind of non-predictable points).\n",
      "Large-scale simulation reveals that methods based upon sets of predictable trajectories compare favourably to those based upon sets of predictable points in terms of the ultimate prediction quality. This approach allowed for making some predictions after the horizon of predictability for the corresponding series.\n",
      "The strategies allow one to work with missing data in quite a natural way.\n",
      "\n",
      "VII. ACKNOWLEDGEMENTS\n",
      "The authors are deeply indebted to Mr. Joel Cumberland, HSE for the manuscript proof-reading and language editing. Authors are indebted to colleagues from Supercomputer Modelling Unit, HSE for access to the supercomputer and valuable advice.\n",
      "The article was prepared within the framework of the HSE University Basic Research Program.\n",
      "\n",
      "VIII. AUTHORS’ CONTRIBUTIONS.\n",
      "\n",
      "The contributors of this paper are as follows:\n",
      "    • Vasilii A. Gromov: conceptualization, methodology; project administration; supervision; writing - original draft;\n",
      "    • Philip S. Baranov: investigation, software, resources, formal analysis writing, review & editing;\n",
      "    • Alexandr Yu. Tsybakin: investigation, software, formal analysis.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
