Abstract
This paper shows that the dynamics of nonlinear systems that produce complex time series can be
captured in a model system. The model system is an artificial neural network, trained with back-
propagation, in a multi-step prediction framework. Results from the Mackey-Glass (D=30) will be
presented to corroborate our claim. Our final intent is to study the applicability of the method to
the electroencephalogram, but first several important questions must be answered to guarantee ap-
propriate modeling.

Introduction

Can complex signals such as the electroencephalogram (EEG) be predicted? In the past, prediction
has been utilized in EEG analysis to develop linear autoregressive models of the EEG signal. Lin-
ear modeling of a signal as complex as the EEG is an approximation that does not have any neu-
rophysiologic plausibility. AR modeling had some clinical applications [Bourne and Jansen], but
in our opinion did not provide any breakthrough for brain theory. Recently the EEG has been an-
alyzed from a different perspective, as the output of a nonlinear dynamical system. Dynamic in-
variants (correlation dimension and Lyapunov exponents) have been utilized to measure the
complexity of the EEG signal and correlate it with behavior states. It is too early to know the clin-
ical value of this technique [1], [2], but it is worth noting that dynamical modeling has the potential
to enhance our understanding about brain function. The neuron and the cell assembly are nonlinear
dynamical systems, so the identification of their dynamical regimens are fundamental aspects to
advance hypotheses about brain function at a level of abstraction that can be tested with neurophys-
iological experimentation. However, the dynamical analysis should not be limited to the computa-
tion of these invariants of motion as seems to be the majority of work in dynamical analysis of the
EEG. First, because it is dubious if one can ever answer the question about the genesis of brain
signals (i.e noise versus complex nonlinear dynamics) through the computation of correlation di-
mension or Lyapunov exponents. Second, because these invariants only describe global properties
of the system that produced the signal (number of equations necessary for modeling and largest
eigenvalue of the Jacobian) what is insufficient knowledge if one wants to attack the inverse prob-
lem, i.e. to obtain artificial models that display the same dynamics properties.

In this paper we use nonlinear dynamic theory to address another question: can prediction of a cha-
otic signal (here defined as a signal generated by a nonlinear dynamical system with a low dimen-
sion attractor) provide any knowledge about the system that produce the time series? It is known
that the dynamical system is a map that relates the current point with the previous points in state
space [3]. Therefore, when a model system predicts the next point in state space, it is attempting to

identify the map that characterizes the original dynamical system. The problem is that the map is
not guarantee to be unique when one uses finite number of state space points. The other problem
is that the map may be very complex and nonlinear, therefore there is no guarantee to identify the
map with sufficient accuracy through prediction.

This paper shows that taking advantage of the nonlinear mapping capabilities of artificial neural
networks and their adaptability, it is possible to identify the dynamics of nonlinear dynamical sys-
tems using the prediction framework on a segment of the time series. For EEG analysis this means
that one will be able to obtain the dynamical map of the system that produced the signal collected
by the electrode (if the signal is in fact produced by a low dimensional attractor). One will then
obtain a model of the dynamics from which a wealth of questions could be asked, which include
the dimension and the full Lyapunov exponents. But the path to reach this goal seems a long one,
and here we will be reporting on the initial steps. We will show how to perform dynamical model-
ing with artificial neural networks and will present some results with a complex time series, the
Mackey-Glass system.

The accepted methodology to perform prediction finds its roots in the optimal filtering problem
discussed and solved by Wiener [4]. Central to this formalism is the concept of minimization of
the mean square error between the desired signal (in the prediction case, a future point of the time
series) and the output of a system fed by the same time series (Figure 1a). Basically the prediction
of the future is obtained by weighting appropriately the information contained in the recent past of
the signal. The use of nonlinear models opens the door to study prediction of chaotic time series.
The subject has recently spurred a lot of interest in the literature, but the bulk of the work is restrict-
ed to the comparison of predictors [5], [6], [7]. In order to model the dynamics of the system that
produced the signal, the first step is to reconstruct the attractor of the system that produced the sig-
nal using Takens embedding theorem and train an artificial neural network how to long term pre-
dict the time series. In [8] it is shown that linear predictors, when used as autonomous systems, are
not able to long term predict a chaotic time series. We utilize here feedforward multilayer artificial
neural networks (ANN), trained with the mean square error between the desired and predicted sam-
ple, to obtain a mapping that when iterated would explicitly represent the trajectories.


Dynamical modeling with ANNs

Due to imperfect prediction and the existence of positive Lyapunov exponents, the prediction of

the future of a chaotic signal will inexorably diverge from the original signal. Therefore in these
applications the ANN predictor works on a segment of the signal to predict x points in the future,
and must be reset. The process is then iterated on an immediately following segment. The ANN is
taught to predict the next point of the time series using the conventional formalism of mean square
error minimization (Figure 1a). In the literature [2], the established criterion to compare one step
predictors is based on the normalized average prediction error is the normalizing factor, f̂N is the estimation of the
                              
smooth map (dynamical system), x(n) is the time series, M is the number of points utilized for the
estimation, k is the model order, and τ is a delay parameter. The ANN during learning is trained
with one-step prediction errors. For multi-step prediction the previously predicted points are uti-
lized to create the estimate of the smooth map so, in the argument of f̂N , the x(n) are progressively
substituted by x̂ ( n ) , the predicted samples. We called this error the long-term prediction error,
and in this paper we set M on the order of twice the value of the attractor’s embedding dimension.
The long term prediction error is also utilized to compare multi-step predictors [5] [6].

Figure 1 shows how the ANN is utilized to create a model for the chaotic time series. First the ANN
parameters will be obtained through prediction in a training configuration (Figure 1a). If the ANN
is able to learn exactly the mapping then we can expect that the test model configuration, when
properly initialized, will produce a time series that will match the original time series. Here we will
say that the dynamics have been captured if the dynamical invariants of the time series generated
by the test model (Figure 1b), which is an autonomous dynamical system match the ones estimated
from the original time series.


Results

In the experiments we utilized the time delay neural network (TDNN) adapted with backpropaga-
tion [9] shown in Figure 2 to predict the Mackey Glass time series, with delay ∆=30, and a sam-
pling period of T=6sec, using fourth order Runga-Kutta integration. The topology of the multilayer
network (Figure 2) is 7 input units, 12 units in the first hidden layer, 16 units in the second hidden
layer, and one unit in the output layer. The processing element of the output layer was linear. The
remaining processing elements utilized the sigmoid nonlinearity [9].

The topology for the TDNN was arrived after trial and error, except for the number of units in the
input layer. The number of input elements should be associated with the embedding dimension of
the attractor. Under these conditions the elements of the first hidden layer will be receiving activa-
tions from the components of the reconstructed points in the embedding space of the attractor, ac-
cording to Takens embedding method [10]. We estimated the embedding dimension of the
Mackey-Glass equation as being seven, by the Grassberger and Proccacia algorithm [11]. Mead et.
al. [6] also recommends that the same method proposes 6 input units. In these results we have not
controlled τ, the delay parameter of the Takens’ embedding, so the sampling gives by default τ=1.


This parameter may affect the learning speed of the neural network.
TDNN was trained to predict the next point with a learning rate of 0.01 in the backpropagation al-
gorithm, until the differential error between two iteration felt below????. The results were obtained
over a test set of 500 points of the Mackey-Glass system. The training set was a different set of
3,500 points of the time series.The correlation dimension and the largest Lyapunov exponents were
computed using in house implementations of the Grassberger and Proccacia algorithm and the
Wolf’s algorithm [12] respectively. This package was developed in Mathematica for the NeXT
computer.


The long term prediction error has been utilized successfully in the rating of predictors because it
indicates, for a given time series and for a given prediction interval, how well the predictor output
is approximating the original time signal. The smaller the long term prediction error, the better is
the approximation of the mapping achieved by the ANN. Figure 3 shows that TDNN is able to fol-
low with a very small error the Mackey -Glass time series up to 45 steps. The prediction error index
slowly degrades with the prediction step. In fact the error has an exponential characteristic shown
in Figure 4, that can be expected from the scaling laws of iterative predictors [3]. In order to keep
a reasonable match between the predicted and original time series, the predictor is normally reset
(here after 45 points).


At the present level of understanding [8], the prediction error indices (both short and long term) do
not seem to help answer the original question of finding a criterion to establish if the ANN was able
to learn the dynamic model. Therefore we resorted to the estimation of the dynamical invariants of
motion from the predicted model. With the weights determined after training (using the indicated
criterion to stop the training), the ANN was initialized with 7 consecutive points of the Mackey-
Glass time series, and the predictor (Figure 1b) iterated for 5,000 points. Figure 5b shows 100
points of the generated time series, which resembles very much the original Mackey-Glass series.


There are global visual waveform features that remain the same, which suggest that the two time
series represent two distinct trajectories on the same attractor. This similarity persists for the
length of our simulation. Notice that the long-term prediction error between the two time series is
very large which could be interpreted as a bad predictive model.

We also computed the spectrum of 4,096 segments of the original and generated time series (using
the Bartlett estimator with 256 points and 16 averages). The spectrum of the generated time series
(Figure 6) seems to follow very closely the spectrum of the original time series, only with minor
differences in the fine structure of the spectrum. Although these plots lead us to believe that the
dynamics have been reasonably captured, we would like to quantify numerically the matching be-
tween the dynamics of the two systems.
That is the reason we propose to compute the correlation dimension and the largest Lyapunov ex-
ponent. In Figure 7 the correlation integral map (CIM) and its slope are depicted both for the orig-
inal and predicted time series. Notice that the scaling region of the CIM curves of the generated


time series is smaller than for the original dynamics, but it is still wide, showing a good saturation
region. The value of the correlation dimension is defined as the slope of the CIM curves for at least
3 consecutive embeddings. In this case the correlation dimension for the predicted time series is
2.25 for the region 4<r<6. The correlation dimension for the time original time series is 2.45 for
3<r<6. We also have computed the largest Lyapunov exponent- lexp using Wolf’s procedure (Fig-
ure 8).
The estimated value for the generated time series is 0.008, of the same order of magnitude but
smaller than for the original time series (0.011). Notice that both estimates stabilize after 10,000
seconds. From the observation of the state space portraits we attribute this smaller value of lexp to
the higher dispersion of the trajectories for the generated attractor. Analyzing the dynamical invari-
ants and the state space trajectories, we conclude that the dynamics of the predicted and original
time series are very similar, and that according to our definition, the dynamics of the system that
produced the original time series have been ‘reasonably” captured in the ANN. Notice that with the
ANN we obtain an explicit (an equation) representation of the dynamics.



Discussion and Conclusion

The primary conclusion of this paper is that feedforward neural networks have the potential to cap-
ture the dynamics of nonlinear dynamical systems as was demonstrated for the MacKey-Glass sys-
tem with ∆=30. This opinion is based on the fact that the invariants of motion estimated from the
original and generated (5,000 points) time series are very similar (2.45 versus 2.25 and 0.011 ver-
sus 0.008, for the correlation dimension and Lyapunov exponent respectively). Notice that during
the development of the methodology there was no restrictions placed on the type of mapping that
can be learned by the ANN, besides being a dynamical system. Therefore it is foreseeable that the
same methodology can be applied to other time series generated by nonlinear dynamical systems,
and eventually the EEG. However, there are a few questions that require further investigation be-
fore attempting more complex signals.

Probably the most important one deals with the architecture of the ANN and the training procedure.
The ANN architecture is associated with the number of degrees of freedom of the system and also
with the precision that can be achieved in the mapping. Presently in the ANN literature there is no
generally accepted procedure to choose an ANN topology given the precision one wants to achieve
in the mapping. Therefore, one must resort to some form of constructive [13] or destructive [14]
procedures to reach a sufficiently good architecture. The other problem relates to the criterion for
learning. The mean square error (L2 norm) is not the best criterion to match a trajectory in state
space, because large errors for small intervals are tolerated (very much like the Gibb’s phenomena
in Fourier series). Unfortunately, other norms to train the network have been difficult to derive. We
are presently testing a more constrained L2 norm, i.e. we try to fit several points at the same time
instead of just the next point. Still another problem that must be faced deals with the different time
scales present in the time series one wants to predict. It is known that learning different time scales
is a difficult problem, because the network may get confused when the short term and the long term
past may contradict. These issues have to be dealt with to guarantee robust dynamical modeling.

